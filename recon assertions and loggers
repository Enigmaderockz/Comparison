import csv
import shutil
import sys
import os
import pandas as pd
import concurrent.futures
import logging
import datetime

# Logger configuration
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Create a file handler for the logger
log_file = f"recon_{datetime.date.today()}.log"
file_handler = logging.FileHandler(log_file)
file_handler.setLevel(logging.DEBUG)

# Create a console handler for the logger
console_handler = logging.StreamHandler()
console_handler.setLevel(logging.ERROR)

# Create a formatter and add it to the handlers
formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
file_handler.setFormatter(formatter)
console_handler.setFormatter(formatter)

# Add the handlers to the logger
logger.addHandler(file_handler)
logger.addHandler(console_handler)

def fetch_delimiter(file_path):
    _, file_extension = os.path.splitext(file_path)
    if file_extension == ".csv" or file_extension == ".dat":
        with open(file_path, "r") as file:
            dialect = csv.Sniffer().sniff(file.read(102400))
            return dialect.delimiter

def read_mapping(file):
    with open(file, "r") as f:
        mapping = {}
        for line in f:
            cols = line.strip().split(":")
            source_cols = cols[0].split(",")
            target_cols = cols[1].split(",")
            for i in range(len(source_cols)):
                mapping[source_cols[i]] = target_cols[i]
    return mapping

def update_column_names(file, column_mapping):
    with open(file, "r") as f:
        reader = csv.DictReader(f, delimiter=delimiter)
        rows = list(reader)
        new_fieldnames = [column_mapping.get(name, name) for name in reader.fieldnames]

    with open(file, "w", newline="") as f:
        writer = csv.DictWriter(f, fieldnames=new_fieldnames, delimiter=delimiter)
        writer.writeheader()
        for row in rows:
            new_row = {
                column_mapping.get(name, name): value for name, value in row.items()
            }
            writer.writerow(new_row)

def create_temp_files(file1, file2, column_mapping=None, buffer_size=1024 * 1024):
    global delimiter
    delimiter = fetch_delimiter(file1)
    logger.info("Delimiter: %s", delimiter)
    # Copy file1
    with open(file1, "rb") as fsrc, open("a_tmp.csv", "wb") as fdst:
        shutil.copyfileobj(fsrc, fdst, buffer_size)

    # Copy file2
    with open(file2, "rb") as fsrc, open("b_tmp.csv", "wb") as fdst:
        shutil.copyfileobj(fsrc, fdst, buffer_size)

    if column_mapping is not None:
        column_mapping = read_mapping("mapping.txt")
        logger.info("Column Mapping: %s", column_mapping)
        update_column_names("a_tmp.csv", column_mapping)
        update_column_names("b_tmp.csv", column_mapping)

def perform_recon_on_files(file1, file2, col_check=None, column_mapping=None):
    logger.info("Col Check: %s", col_check)
    delimiter1 = ","
    delimiter2 = "|"
    if col_check is not None:
        # Read the CSV files
        df1 = pd.read_csv(file1, dtype=str)
        df2 = pd.read_csv(file2, dtype=str)

        col_check_list = col_check.split(",")

        if column_mapping is not None:
            col_check_list = [column_mapping.get(col, col) for col in col_check_list]

        # Create sets for faster lookup
        set1 = set(tuple(row) for row in df1[col_check_list].values)
        set2 = set(tuple(row) for row in df2[col_check_list].values)

        # Find extra records
        extra_records1 = df1[~df1[col_check_list].apply(tuple, axis=1).isin(set2)]
        extra_records2 = df2[~df2[col_check_list].apply(tuple, axis=1).isin(set1)]

        # Write the results to output.csv
        with open("output.csv", "w", newline="") as output_file:
            writer = csv.writer(output_file, delimiter=delimiter1)  # Use ',' as delimiter

            # Write the first set of extra records
            writer.writerow([f"Extra records in {file1} which are not present in {file2} based on " + col_check + ": " + str(len(extra_records1))])
            writer.writerow([])
            if not extra_records1.empty:
                writer.writerow(extra_records1.columns)
                extra_records1 = extra_records1.fillna('')
                writer.writerows(extra_records1.values)
            else:
                writer.writerow([])

            writer.writerow([])

            writer = csv.writer(output_file, delimiter=delimiter2)

            # Write the second set of extra records
            writer.writerow([f"Extra records in {file2} which are not present in {file1} based on " + col_check + ": " + str(len(extra_records2))])
            writer.writerow([])
            if not extra_records2.empty:
                writer.writerow(extra_records2.columns)
                extra_records2 = extra_records2.fillna('')
                writer.writerows(extra_records2.values)
            else:
                writer.writerow([])

        # Update the temporary files
        df1 = df1[df1[col_check_list].apply(tuple, axis=1).isin(set2)]
        df2 = df2[df2[col_check_list].apply(tuple, axis=1).isin(set1)]
        df1.to_csv(file1, index=False, sep=delimiter1)
        df2.to_csv(file2, index=False, sep=delimiter2)

def compare_csv(file1, file2, col_check=None, column_mapping=None):
    try:
        create_temp_files(file1, file2, column_mapping)
        perform_recon_on_files(
            "a_tmp.csv", "b_tmp.csv", col_check=col_check, column_mapping=column_mapping
        )
    except FileNotFoundError as e:
        logger.error("File not found: %s", e.filename)
        raise
    except Exception as e:
        logger.error(str(e))

# Run the compare_csv function with the required parameters
file1 = "a.dat"
file2 = "b.dat"
column_mapping = None
col_check = None

for arg in sys.argv:
    if arg.startswith("col_check="):
        _, value = arg.split("=")
        if value.lower() != "none":
            if value == "*":
                with open(file2, "r") as f:
                    reader = csv.reader(f)
                    col_check = next(reader)
                    col_check = ",".join(col_check)
                    logger.info("Col Check: %s", col_check)
            else:
                col_check = value
    elif arg.startswith("column_mapping="):
        _, value = arg.split("=")
        if value.lower() == "y":
            with open("mapping.txt", "r") as f:
                column_mapping = {}
                lines = f.readlines()
                for line in lines:
                    mapping = line.strip().split(":")
                    if len(mapping) == 2:
                        src_cols = mapping[0].split(",")
                        dest_cols = mapping[1].split(",")
                        for src_col, dest_col in zip(src_cols, dest_cols):
                            column_mapping[src_col] = dest_col
        else:
            column_mapping = None


compare_csv(file1, file2, col_check=col_check, column_mapping=column_mapping)

